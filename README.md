# Emotion-Detector-From-Audio-Input

Overview =>

This project implements a deep learning-based system that classifies emotions from human speech using audio features. The goal is to enhance human-computer interaction by enabling machines to understand emotional cues through speech signals.
The model leverages MFCCs, Chroma vectors, and Mel Spectrograms to train a Multi-Layer Perceptron (MLP), achieving a 74.48% accuracy in classifying 4 core emotions:

ðŸ˜Œ Calm
ðŸ˜Š Happy
ðŸ˜± Fearful
ðŸ¤¢ Disgust

![image](https://github.com/user-attachments/assets/479a5633-3a79-4bd4-b090-98882219d061)

